# Project: Documentation-Grounded Learning Assistant
## Problem Description

Learning from technical documentation is often inefficient because:
- relevant information is scattered across many files
- searching requires knowing exact terminology
- general-purpose chatbots hallucinate or use external knowledge

The goal of this project is to build a documentation-grounded AI assistant that:
- answers questions only based on a given documentation repository
- explicitly searches the provided documents before answering
- helps users learn and self-check their understanding of the material

The system is designed as a learning aid, not a general chatbot.

## System Functionality

The system allows a user to:
- Ask a question related to the documentation
- Retrieve relevant document sections using text-based search
- Generate an answer strictly grounded in the retrieved documents
- Provide references (file paths) to the source material

If no relevant documents are found, the system:
- informs the user
- avoids hallucinated answers


## AI System Development

This project uses an LLM-assisted retrieval workflow:
1. User query is passed to a search component
2. The search component retrieves relevant documentation chunks
3. The retrieved content is injected into the system prompt
4. The language model generates an answer constrained by the provided context

AI tools were also used during development:
- ChatGPT / coding assistants were used to iterate on code structure
- Prompts were refined to reduce hallucination and enforce grounding

MCP (Model Context Protocol) is not implemented in this version but is considered as future work.

## Technologies and Architecture
**Backend**
- Python
- FastAPI (REST API + OpenAPI specification)
- LLM API (reasoning and answer generation)

**Frontend**
- Simple web UI (HTML or Streamlit)
- Single entry point for backend communication

**Search**
- Text-based search over documentation files
- No embeddings used in the initial version to keep behavior interpretabl

**Storage**
No database is used
The system is stateless by design

User
  ↓
Frontend
  ↓
FastAPI Backend
  ↓
Search Engine → Documentation
  ↓
LLM (context-grounded)

**API Contract**

The backend exposes a REST API documented via OpenAPI.
- OpenAPI schema is automatically generated by FastAPI
- The frontend strictly follows this API contract
- Interactive API documentation is available at /docs

**Front-End Implementation**

The front-end:
- provides a single interface for asking questions
- sends all requests through a centralized API client
- displays answers and references returned by the backend

The focus is on clarity and correctness rather than UI complexity.

**Limitations**

- The system does not understand synonyms automatically
- Retrieval is lexical (keyword-based)
- No conversation memory between queries
- No persistent storage

These limitations are intentional to keep the system transparent and debuggable.

**Reproducibility**

To run the project locally:
git clone <repo-url>
cd <repo>
pip install -r requirements.txt
export OPENAI_API_KEY=your_key_here
uvicorn main:app --reload

Then open:
API docs: http://localhost:8000/docs
Frontend: http://localhost:8000/

**Future Work**
- Vector search with embeddings
- Synonym expansion
- MCP-based agent orchestration
- Persistent storage for learning history
- Improved evaluation metrics